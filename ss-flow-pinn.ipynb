{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6u_j73SUfQxG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Physics Informed Neural Networks to Approximate Solution of PDEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 196,
     "status": "ok",
     "timestamp": 1679401692196,
     "user": {
      "displayName": "Roberto Molinaro",
      "userId": "05050284606345065493"
     },
     "user_tz": -60
    },
    "id": "NOPdRWmOfQxI",
    "outputId": "7e48ccf5-8b35-43b2-fa3c-ebf973323f50",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Common'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mCommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NeuralNet, MultiVariatePoly\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      8\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m128\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Common'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from Common import NeuralNet, MultiVariatePoly\n",
    "import time\n",
    "torch.manual_seed(128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kM0O07h0fQxJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Consider the porous 2D incompressible steady-state Stokes equation:\n",
    "$\\newcommand{\\Reynolds}{\\text{Re}}\\renewcommand{\\div}{\\text{div}}$\n",
    "$$\n",
    " -\\nabla \\hat{p} + \\div(\\frac{2}{\\Reynolds} \\epsilon(\\hat{u}))\n",
    " -\\frac{1}{\\Reynolds} \\left(\\frac{5L^2}{2H_t^2} + \\hat{\\alpha}(\\rho)\\right) \\hat{u} = 0\n",
    "$$\n",
    "$$\n",
    "\\div(\\hat{u})= 0\n",
    "$$\n",
    "\n",
    "with constant inflow and boundary conditions\n",
    "$$\n",
    "u_b(0,y) = \\text{const} u_{in}\n",
    "$$\n",
    "\n",
    "$$\n",
    "u_b(x,0)=u_b(x,1)=0, \n",
    "$$\n",
    "\n",
    "We want to obtain an approximate solution of $u : [0,1]\\times[0,1] \\mapsto \\mathbb{R}^2$ with physics informed neural networks (PINNs).\n",
    "\n",
    "To do so, we approximate the underlying solution with a feedforward dense neural network with tunable parameters $\\theta$:\n",
    "\n",
    "$$\n",
    "u_\\theta(x,y) \\approx u(x,y)\n",
    "$$\n",
    "Define the following residuals:\n",
    "\n",
    "   - Interior residual given by,\n",
    "\n",
    "   $$r_{int,0,\\theta}(x, y):= -\\nabla \\hat{p} + \\div(\\frac{2}{\\Reynolds} \\epsilon(\\hat{u}))\n",
    " -\\frac{1}{\\Reynolds} \\left(\\frac{5L^2}{2H_t^2} + \\hat{\\alpha}(\\rho)\\right) \\hat{u}$$\n",
    "   $$r_{int,1,\\theta}(x, y):= \\div(\\hat{u})$$\n",
    "   \n",
    "   - inflow boundary residual given by,\n",
    "   \n",
    "        $$r_{ib,\\theta}(x,y):= u_{\\theta}(x,y)- u_in$$\n",
    "        \n",
    "   - outside boundary residual given by,\n",
    "   \n",
    "        $$r_{ob,\\theta}(x,0):= u_{\\theta}(x,0), r_{ob, \\theta}(x,1) := u_{\\theta}(x,1) \\quad \\forall x \\in [-1,1].$$\n",
    "\n",
    "and compute the corresponding loss functions:\n",
    "\n",
    "$$\n",
    "L_{int,0}(\\theta) = \\int_{[0,1]\\times[0,1]}r_{int,0,\\theta}^2(x, y) dxdy, \\quad\n",
    "L_{int,1}(\\theta) = \\int_{[0,1]\\times[0,1]}r_{int,1,\\theta}^2(x, y) dxdy, \\quad\n",
    "L_{ib}(\\theta) = \\int_{[0,1]}r_{ib,\\theta}^2(0,y) dy, \\quad\n",
    "L_{ob}(\\theta) = \\int_{[0,1]}r_{ob,\\theta}^2(x,0) dx + \\int_{[0,1]}r_{ob,\\theta}^2(x,1) dx\n",
    "$$\n",
    "\n",
    "The loss functions include integrals that can be approximated by suitable quadrature rule. We use quasi Monte-Carlo and accordingly define the following training sets\n",
    "\n",
    "$$\n",
    "S_{int} =\\{z_n\\}, \\quad 1 \\leq n \\leq N_{int},\\quad z_n = (x,y)_n \\in D_T,\n",
    "$$\n",
    "\n",
    "$$\n",
    "S_{ib} =\\{y_n, u_b(0,y_n) \\}, \\quad1 \\leq n \\leq N_{ib}, y_n \\in [0,1],\n",
    "$$\n",
    "\n",
    "$$\n",
    "S_{ob, 0}=\\{x_n, u_0(x_n,0)\\}\\quad  1 \\leq n \\leq N_{ob}, x_n \\in [0,1].\n",
    "$$\n",
    "\n",
    "$$\n",
    "S_{ob, 1}=\\{x_n, u_0(x_n,1)\\}\\quad  1 \\leq n \\leq N_{ob}, x_n \\in [0,1].\n",
    "$$\n",
    "\n",
    "\n",
    "with the training inputs points corresponding to low-discrepancy Sobol sequences.\n",
    "\n",
    "$$\n",
    "L_{int,0}(\\theta) = \\frac{1}{N_{int}}\\sum_{i=1}^{N_{int}}r_{int,1,\\theta}^2(z_n) , \\quad\n",
    "L_{int,1}(\\theta) = \\frac{1}{N_{int}}\\sum_{i=1}^{N_{int}}r_{int,1,\\theta}^2(z_n) , \\quad\n",
    "L_{ib}(\\theta) = \\frac{1}{N_{ib}}\\sum_{i=1}^{N_{ib}}r_{sb,\\theta}^2(y_n,0)\\quad\n",
    "L_{ob}(\\theta) = \\frac{1}{N_{ob}}\\sum_{i=1}^{N_{ob}}r_{ob,\\theta}^2(x_n,0) +\\frac{1}{N_{ob}}\\sum_{i=1}^{N_{ob}}r_{ob,\\theta}^2(x_n,1)\n",
    "$$\n",
    "\n",
    "and solve the following minimization problem\n",
    "\n",
    "$$\n",
    "\\theta^\\ast = argmin_{\\theta} \\Big(L_{int}(\\theta) + \\lambda_u L_u(\\theta)\\Big)\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "L_{int} = L_{int,0} + L_{int,1}, \\quad\n",
    "L_u = L_{ib}(\\theta) + L_{ob}(\\theta)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 220,
     "status": "ok",
     "timestamp": 1679403218293,
     "user": {
      "displayName": "Roberto Molinaro",
      "userId": "05050284606345065493"
     },
     "user_tz": -60
    },
    "id": "HgRzd8_wfQxK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Pinns:\n",
    "    def __init__(self, n_int_, n_ib_, n_ob_):\n",
    "        self.n_int = n_int_\n",
    "        self.n_ib = n_ob_\n",
    "        self.n_ob = n_ib_\n",
    "        \n",
    "\n",
    "        # Extrema of the solution domain (x,y) in [0,1]x[0,1]\n",
    "        self.domain_extrema = torch.tensor([[0, 1],[0, 1]])\n",
    "\n",
    "        # Number of space dimensions\n",
    "        self.space_dimensions = 2\n",
    "\n",
    "        # Parameter to balance role of data and PDE\n",
    "        self.lambda_u = 10\n",
    "        \n",
    "        self.T_inlet = 293.15  # K\n",
    "\n",
    "        self.channel_thickness = 380e-6  # m\n",
    "        self.Ht = 0.5 * self.channel_thickness\n",
    "\n",
    "        self.substrate_thickness = 150e-6  # m\n",
    "        self.Hs = 0.5 * self.substrate_thickness\n",
    "\n",
    "        self.conductivity_fluid = 0.598\n",
    "        self.conductivity_substrate = 149\n",
    "        self.viscosity_fluid = 1.004e-3\n",
    "        self.capacity_fluid = 4180\n",
    "        self.density_fluid = 998\n",
    "\n",
    "        self.applied_pressure = 100000 * 5.0 / 4.0  # kg m^-1 s^-2\n",
    "        self.L = 0.001  # m\n",
    "        self.U = np.sqrt(self.applied_pressure / self.density_fluid)  # m/s\n",
    "\n",
    "\n",
    "        self.nu = self.viscosity_fluid / self.density_fluid\n",
    "        self.rho = self.load_rho()\n",
    "\n",
    "        self.Re = self.L * self.U / self.nu\n",
    "        #Prandtl_number = fd.Constant(capacity_fluid * viscosity_fluid / conductivity_fluid)\n",
    "        self.nondim_Ht = self.Ht / self.L\n",
    "        self.nondim_Hs = self.Hs / self.L\n",
    "        \n",
    "        self.qk = 1 #this change in each optimization steps\n",
    "        self.alpha_f = 0\n",
    "        self.alpha_s = 5*self.L**2/(2*self.Ht**2)\n",
    "\n",
    "        # F Dense NN to approximate the solution of the underlying equation\n",
    "        self.approximate_solution = NeuralNet(input_dimension=self.domain_extrema.shape[0], output_dimension=4,\n",
    "                                              n_hidden_layers=4,\n",
    "                                              neurons=20,\n",
    "                                              regularization_param=0.,\n",
    "                                              regularization_exp=2.,\n",
    "                                              retrain_seed=42)\n",
    "        '''self.approximate_solution = MultiVariatePoly(self.domain_extrema.shape[0], 3)'''\n",
    "\n",
    "        # Generator of Sobol sequences\n",
    "        self.sobol = torch.quasirandom.SobolEngine(dimension=self.domain_extrema.shape[0])\n",
    "                \n",
    "\n",
    "        # Training sets S_ib, S_ob, S_int as torch dataloader\n",
    "        self.training_set_ib, self.training_set_ob, self.training_set_int = self.assemble_datasets()\n",
    "    \n",
    "    def load_rho(self):\n",
    "        #file_path = \"/mnt/c/Users/bonvi/Documents/simulation_hack/simulation_hackaton_eth-rafael/simulation_hackaton_eth-rafael/formatted_data0/\"\n",
    "        file_path = \"C:\\\\Users\\\\bonvi\\\\Documents\\\\simulation_hack\\\\simulation_hackaton_eth-rafael\\\\simulation_hackaton_eth-rafael\\\\formatted_data0\\\\\"\n",
    "        file = file_path + 'results-0.h5_rho.out'\n",
    "        return np.loadtxt(file)\n",
    "\n",
    "    ################################################################################################\n",
    "    # Function to linearly transform a tensor whose value are between 0 and 1\n",
    "    # to a tensor whose values are between the domain extrema\n",
    "    def convert(self, tens):\n",
    "        assert (tens.shape[1] == self.domain_extrema.shape[0])\n",
    "        return tens * (self.domain_extrema[:, 1] - self.domain_extrema[:, 0]) + self.domain_extrema[:, 0]\n",
    "\n",
    "    # Initial condition to solve the inflow boundary condition\n",
    "    def initial_condition(self, x):\n",
    "        return (1,0)\n",
    "\n",
    "    # Exact solution NOT APPLICABLE\n",
    "#     def exact_solution(self, inputs):\n",
    "#         t = inputs[:, 0]\n",
    "#         x = inputs[:, 1]\n",
    "\n",
    "#         u = -torch.exp(-np.pi ** 2 * t) * torch.sin(np.pi * x)\n",
    "#         return u\n",
    "\n",
    "    ################################################################################################\n",
    "    # Function returning the input-output tensor required to assemble the training set S_ib corresponding to the vertical boundary\n",
    "    def add_inflow_boundary_points(self):\n",
    "        y0 = self.domain_extrema[0, 0]\n",
    "        input_ib = self.convert(self.soboleng.draw(self.n_ib))\n",
    "        input_ib[:, 0] = torch.full(input_ib[:, 0].shape, t0)\n",
    "        output_ib = self.initial_condition(input_ib[:, 1]).reshape(0, 1)\n",
    "\n",
    "        return input_ib, output_ib\n",
    "\n",
    "    # Function returning the input-output tensor required to assemble the training set S_ob corresponding to the spatial boundary\n",
    "    def add_horizontal_boundary_points(self):\n",
    "        x0 = self.domain_extrema[1, 0]\n",
    "        xL = self.domain_extrema[1, 1]\n",
    "\n",
    "        input_ob = self.convert(self.soboleng.draw(self.n_ob))\n",
    "\n",
    "        input_ob_0 = torch.clone(input_ob)\n",
    "        input_ob_0[:, 1] = torch.full(input_sb_0[:, 1].shape, x0)\n",
    "\n",
    "        input_ob_L = torch.clone(input_ob)\n",
    "        input_ob_L[:, 1] = torch.full(input_sb_L[:, 1].shape, xL)\n",
    "\n",
    "        output_ob_0 = torch.zeros((input_ob.shape[0], 1))\n",
    "        output_ob_L = torch.zeros((input_ob.shape[0], 1))\n",
    "\n",
    "        return torch.cat([input_ob_0, input_ob_L], 0), torch.cat([output_ob_0, output_ob_L], 0)\n",
    "\n",
    "    #  Function returning the input-output tensor required to assemble the training set S_int corresponding to the interior domain where the PDE is enforced\n",
    "    def add_interior_points(self):\n",
    "        input_int = self.convert(self.soboleng.draw(self.n_int))\n",
    "        output_int = torch.zeros((input_int.shape[0], 1))\n",
    "        return input_int, output_int\n",
    "\n",
    "    # Function returning the training sets S_ib, S_ob, S_int as dataloader\n",
    "    def assemble_datasets(self):\n",
    "        input_ib, output_ib = self.add_inflow_boundary_points()  # S_ib\n",
    "        input_ob, output_ob = self.add_horizontal_boundary_points()   # S_ob\n",
    "        input_int, output_int = self.add_interior_points()         # S_int\n",
    "\n",
    "        training_set_ib = DataLoader(torch.utils.data.TensorDataset(input_ib, output_ib), batch_size=self.space_dimensions*self.n_ib, shuffle=False)\n",
    "        training_set_ob = DataLoader(torch.utils.data.TensorDataset(input_ob, output_ob), batch_size=2*self.n_ob, shuffle=False)\n",
    "        training_set_int = DataLoader(torch.utils.data.TensorDataset(input_int, output_int), batch_size=self.n_int, shuffle=False)\n",
    "\n",
    "        return training_set_ib, training_set_ob, training_set_int\n",
    "\n",
    "    ################################################################################################\n",
    "    # Function to compute the terms required in the definition of the INFLOW boundary residual\n",
    "    def apply_inflow_condition(self, input_ib):\n",
    "        u_pred_ib = self.approximate_solution(input_ib)\n",
    "        return u_pred_ib\n",
    "\n",
    "    # Function to compute the terms required in the definition of the HORIZONTAL boundary residual\n",
    "    def apply_boundary_conditions(self, input_ob):\n",
    "        u_pred_ob = self.approximate_solution(input_ob)\n",
    "\n",
    "        return u_pred_ob\n",
    "\n",
    "    def alpha(self,rho):\n",
    "        return (1-rho)/(1+self.qk*rho)*(self.alpha_s - self.alpha_f)+self.alpha_f\n",
    "    \n",
    "    def interpolate_rho(self,input_int):\n",
    "        input_int = int(input_int.detach() * 100)\n",
    "        rho = np.zeros((input_int.shape[0],1))\n",
    "        for i in range(input_int.shape[0]):\n",
    "            x = input_int[i,0]\n",
    "            y = input_int[i,1]\n",
    "            rho[i] = self.rho[x,y]\n",
    "        return rho\n",
    "        \n",
    "    def compute_pde_residual(self, input_int):\n",
    "        rho = interpolate_rho(input_int)\n",
    "        input_int.requires_grad = True # save gradients\n",
    "        \n",
    "        u = self.approximate_solution(input_int)[:,0:2]\n",
    "        p = self.approximate_solution(input_int)[:,2:4] \n",
    "\n",
    "        # grad compute the gradient of a \"SCALAR\" function L with respect to some input nxm TENSOR Z=[[x1, y1],[x2,y2],[x3,y3],...,[xn,yn]], m=2\n",
    "        # it returns grad_L = [[dL/dx1, dL/dy1],[dL/dx2, dL/dy2],[dL/dx3, dL/dy3],...,[dL/dxn, dL/dyn]]\n",
    "        # Note: pytorch considers a tensor [u1, u2,u3, ... ,un] a vectorial function\n",
    "        # whereas sum_u = u1 + u2 + u3 + u4 + ... + un as a \"scalar\" one\n",
    "\n",
    "        # In our case ui = u(xi), therefore the line below returns:\n",
    "        # grad_u = [[dsum_u/dx1, dsum_u/dy1],[dsum_u/dx2, dsum_u/dy2],[dsum_u/dx3, dL/dy3],...,[dsum_u/dxm, dsum_u/dyn]]\n",
    "        # and dsum_u/dxi = d(u1 + u2 + u3 + u4 + ... + un)/dxi = d(u(x1) + u(x2) u3(x3) + u4(x4) + ... + u(xn))/dxi = dui/dxi\n",
    "        grad_u = torch.autograd.grad(u.sum(), input_int, create_graph=True)[0]\n",
    "        #print(grad_u[0].shape)\n",
    "        #grad_u = grad_u[0]\n",
    "        grad_u_x = grad_u[:, 0]\n",
    "        grad_u_y = grad_u[:, 1]\n",
    "        grad_u_xx = torch.autograd.grad(grad_u_x.sum(), input_int, create_graph=True)[0][:, 0]\n",
    "        grad_u_yy = torch.autograd.grad(grad_u_y.sum(), input_int, create_graph=True)[0][:, 1]\n",
    "        \n",
    "\n",
    "\n",
    "        grad_p = torch.autograd.grad(p.sum(), input_int, create_graph=True)[0]\n",
    "        grad_p_x = grad_p[:,0]\n",
    "        grad_p_y = grad_p[:,1]\n",
    "        \n",
    "        nabla_p = grad_p_x + grad_p_y\n",
    "        div_term = 2*self.Re*(grad_u_xx + grad_u_yy)\n",
    "        drag_term = 1/self.Re * (5 * self.L**2 / (2 * self.Ht**2) + self.alpha(rho)) * u\n",
    "        \n",
    "        residual = -nabla_p + div_term - drag_term\n",
    "        return residual.reshape(-1, )\n",
    "    \n",
    "    \n",
    "    # Function to compute the incompressibility term residuals on the interior\n",
    "    def compute_incompress_residual(self, input_int):\n",
    "        input_int.requires_grad = True\n",
    "        u = self.approximate_solution(input_int)\n",
    "\n",
    "        # grad compute the gradient of a \"SCALAR\" function L with respect to some input nxm TENSOR Z=[[x1, y1],[x2,y2],[x3,y3],...,[xn,yn]], m=2\n",
    "        # it returns grad_L = [[dL/dx1, dL/dy1],[dL/dx2, dL/dy2],[dL/dx3, dL/dy3],...,[dL/dxn, dL/dyn]]\n",
    "        # Note: pytorch considers a tensor [u1, u2,u3, ... ,un] a vectorial function\n",
    "        # whereas sum_u = u1 + u2 + u3 + u4 + ... + un as a \"scalar\" one\n",
    "\n",
    "        # In our case ui = u(xi), therefore the line below returns:\n",
    "        # grad_u = [[dsum_u/dx1, dsum_u/dy1],[dsum_u/dx2, dsum_u/dy2],[dsum_u/dx3, dL/dy3],...,[dsum_u/dxm, dsum_u/dyn]]\n",
    "        # and dsum_u/dxi = d(u1 + u2 + u3 + u4 + ... + un)/dxi = d(u(x1) + u(x2) u3(x3) + u4(x4) + ... + u(xn))/dxi = dui/dxi\n",
    "        \n",
    "        grad_u = torch.autograd.grad(u.sum(), input_int, create_graph=True)[0]\n",
    "        grad_u_x = grad_u[:, 0]\n",
    "        grad_u_y = grad_u[:, 1]\n",
    "        div_u = grad_u_x + grad_u_y\n",
    "    \n",
    "        return div_u.reshape(-1, )\n",
    "\n",
    "    # Function to compute the total loss (weighted sum of inflow boundary loss, horizontal boundary loss and interior loss)\n",
    "    def compute_loss(self, inp_train_ib, u_train_ib, inp_train_ob, u_train_ob, inp_train_int, verbose=True):\n",
    "        u_pred_ib = self.apply_inflow_condition(inp_train_ib)\n",
    "        u_pred_ob = self.apply_boundary_conditions(inp_train_ob)\n",
    "        \n",
    "\n",
    "        assert (u_pred_ib.shape[1] == u_train_ib.shape[1])\n",
    "        assert (u_pred_ob.shape[1] == u_train_ob.shape[1])\n",
    "\n",
    "\n",
    "        r_int = self.compute_pde_residual(inp_train_int)\n",
    "        r_inc = self.compute_incompress_residual(inp_train_int)\n",
    "        r_ib = u_train_ib - u_pred_ib\n",
    "        r_ob = u_train_ob - u_pred_ob\n",
    "\n",
    "        loss_ib = torch.mean(abs(r_ib) ** 2)\n",
    "        loss_ob = torch.mean(abs(r_ob) ** 2)\n",
    "        loss_int = torch.mean(abs(r_int) ** 2)\n",
    "        loss_inc = torch.mean(abs(r_inc) ** 2)\n",
    "\n",
    "        loss_u = loss_ib + loss_ob\n",
    "\n",
    "        loss = torch.log10(self.lambda_u * (loss_ib + loss_ob) + loss_int + loss_inc)\n",
    "        if verbose: print(\"Total loss: \", round(loss.item(), 4), \"| PDE Loss: \", round(torch.log10(loss_u).item(), 4), \"| Function Loss: \", round(torch.log10(loss_int + loss_inc).item(), 4))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    ################################################################################################\n",
    "    def fit(self, num_epochs, optimizer, verbose=True):\n",
    "        history = list()\n",
    "\n",
    "        # Loop over epochs\n",
    "        for epoch in range(num_epochs):\n",
    "            if verbose: print(\"################################ \", epoch, \" ################################\")\n",
    "\n",
    "            for j, ((inp_train_ib, u_train_ib), (inp_train_ob, u_train_ob), (inp_train_int, u_train_int)) in enumerate(zip(self.training_set_sb, self.training_set_tb, self.training_set_int)):\n",
    "                def closure():\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = self.compute_loss(inp_train_ib, u_train_ib, inp_train_ob, u_train_ob, inp_train_int, verbose=verbose)\n",
    "                    loss.backward()\n",
    "\n",
    "                    history.append(loss.item())\n",
    "                    return loss\n",
    "\n",
    "                optimizer.step(closure=closure)\n",
    "\n",
    "        print('Final Loss: ', history[-1])\n",
    "\n",
    "        return history\n",
    "\n",
    "    ################################################################################################\n",
    "    def plotting(self):\n",
    "        inputs = self.soboleng.draw(100000)\n",
    "        inputs = self.convert(inputs)\n",
    "\n",
    "        output = self.approximate_solution(inputs).reshape(-1, )\n",
    "#         exact_output = self.exact_solution(inputs).reshape(-1, )\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(16, 8), dpi=150)\n",
    "#         im1 = axs[0].scatter(inputs[:, 1].detach(), inputs[:, 0].detach(), c=exact_output.detach(), cmap=\"jet\")\n",
    "#         axs[0].set_xlabel(\"x\")\n",
    "#         axs[0].set_ylabel(\"t\")\n",
    "#         plt.colorbar(im1, ax=axs[0])\n",
    "#         axs[0].grid(True, which=\"both\", ls=\":\")\n",
    "        im2 = axs[0].scatter(inputs[:, 1].detach(), inputs[:, 0].detach(), c=output.detach(), cmap=\"jet\")\n",
    "        axs[0].set_xlabel(\"x\")\n",
    "        axs[0].set_ylabel(\"t\")\n",
    "        plt.colorbar(im2, ax=axs[0])\n",
    "        axs[0].grid(True, which=\"both\", ls=\":\")\n",
    "#         axs[0].set_title(\"Exact Solution\")\n",
    "        axs[0].set_title(\"Approximate Solution\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "#         err = (torch.mean((output - exact_output) ** 2) / torch.mean(exact_output ** 2)) ** 0.5 * 100\n",
    "#         print(\"L2 Relative Error Norm: \", err.item(), \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 221,
     "status": "ok",
     "timestamp": 1679403192490,
     "user": {
      "displayName": "Roberto Molinaro",
      "userId": "05050284606345065493"
     },
     "user_tz": -60
    },
    "id": "M3ug4ztBfQxM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NeuralNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m n_sb \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[0;32m      8\u001b[0m n_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[1;32m---> 10\u001b[0m pinn \u001b[38;5;241m=\u001b[39m \u001b[43mPinns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_sb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 49\u001b[0m, in \u001b[0;36mPinns.__init__\u001b[1;34m(self, n_int_, n_ib_, n_ob_)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha_s \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mHt\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# F Dense NN to approximate the solution of the underlying equation\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapproximate_solution \u001b[38;5;241m=\u001b[39m \u001b[43mNeuralNet\u001b[49m(input_dimension\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain_extrema\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], output_dimension\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m     50\u001b[0m                                       n_hidden_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m     51\u001b[0m                                       neurons\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m     52\u001b[0m                                       regularization_param\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.\u001b[39m,\n\u001b[0;32m     53\u001b[0m                                       regularization_exp\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.\u001b[39m,\n\u001b[0;32m     54\u001b[0m                                       retrain_seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''self.approximate_solution = MultiVariatePoly(self.domain_extrema.shape[0], 3)'''\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Generator of Sobol sequences\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'NeuralNet' is not defined"
     ]
    }
   ],
   "source": [
    "# Solve the heat equation:\n",
    "# u_t = u_xx, (t,x) in [0, 0.1]x[-1,1]\n",
    "# with zero dirichlet BC and\n",
    "# u(x,0)= -sin(pi x)\n",
    "\n",
    "n_int = 256\n",
    "n_sb = 64\n",
    "n_tb = 64\n",
    "\n",
    "pinn = Pinns(n_int, n_sb, n_tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 859
    },
    "executionInfo": {
     "elapsed": 1054,
     "status": "ok",
     "timestamp": 1679401764229,
     "user": {
      "displayName": "Roberto Molinaro",
      "userId": "05050284606345065493"
     },
     "user_tz": -60
    },
    "id": "l4gxwi51fQxM",
    "outputId": "048a7506-4f92-447d-e58d-e39f05548b23",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the input training points\n",
    "input_sb_, output_sb_ = pinn.add_spatial_boundary_points()\n",
    "input_tb_, output_tb_ = pinn.add_temporal_boundary_points()\n",
    "input_int_, output_int_ = pinn.add_interior_points()\n",
    "\n",
    "plt.figure(figsize=(16, 8), dpi=150)\n",
    "plt.scatter(input_sb_[:, 1].detach().numpy(), input_sb_[:, 0].detach().numpy(), label=\"Boundary Points\")\n",
    "plt.scatter(input_int_[:, 1].detach().numpy(), input_int_[:, 0].detach().numpy(), label=\"Interior Points\")\n",
    "plt.scatter(input_tb_[:, 1].detach().numpy(), input_tb_[:, 0].detach().numpy(), label=\"Initial Points\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"t\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 256,
     "status": "ok",
     "timestamp": 1679403222417,
     "user": {
      "displayName": "Roberto Molinaro",
      "userId": "05050284606345065493"
     },
     "user_tz": -60
    },
    "id": "0CPnHdKtfQxN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "optimizer_LBFGS = optim.LBFGS(pinn.approximate_solution.parameters(),\n",
    "                              lr=float(0.5),\n",
    "                              max_iter=50000,\n",
    "                              max_eval=50000,\n",
    "                              history_size=150,\n",
    "                              line_search_fn=\"strong_wolfe\",\n",
    "                              tolerance_change=1.0 * np.finfo(float).eps)\n",
    "optimizer_ADAM = optim.Adam(pinn.approximate_solution.parameters(),\n",
    "                            lr=float(0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 452280,
     "status": "ok",
     "timestamp": 1679403677083,
     "user": {
      "displayName": "Roberto Molinaro",
      "userId": "05050284606345065493"
     },
     "user_tz": -60
    },
    "id": "qub-M5jqfQxN",
    "outputId": "6cf2b62f-f8f2-4e80-b0c4-c3a736fa93eb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hist = pinn.fit(num_epochs=n_epochs,\n",
    "                optimizer=optimizer_LBFGS,\n",
    "                verbose=True)\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "plt.grid(True, which=\"both\", ls=\":\")\n",
    "plt.plot(np.arange(1, len(hist) + 1), hist, label=\"Train Loss\")\n",
    "plt.xscale(\"log\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 18306,
     "status": "ok",
     "timestamp": 1679403731292,
     "user": {
      "displayName": "Roberto Molinaro",
      "userId": "05050284606345065493"
     },
     "user_tz": -60
    },
    "id": "3HG9DQM5fQxN",
    "outputId": "7aafd9d3-4ccb-40eb-fd91-185602c79aba",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pinn.plotting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1679393536192,
     "user": {
      "displayName": "Roberto Molinaro",
      "userId": "05050284606345065493"
     },
     "user_tz": -60
    },
    "id": "U_q8DqdznK6z"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
